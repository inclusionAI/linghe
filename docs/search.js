window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "linghe", "modulename": "linghe", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.attn", "modulename": "linghe.attn", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.attn.la", "modulename": "linghe.attn.la", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.attn.mla", "modulename": "linghe.attn.mla", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental", "modulename": "linghe.experimental", "kind": "module", "doc": "<p>kernels should be run with torch above 2.9.0</p>\n"}, {"fullname": "linghe.experimental.demb", "modulename": "linghe.experimental.demb", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental.demb.triton_tp_embedding_lookup_backward", "modulename": "linghe.experimental.demb", "qualname": "triton_tp_embedding_lookup_backward", "kind": "function", "doc": "<p>inplace update embedding weight gradient</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output</li>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>g_ptr:</strong>  data_ptr of embedding weight gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">g_ptr</span>, </span><span class=\"param\"><span class=\"n\">vocab_size</span>, </span><span class=\"param\"><span class=\"n\">hdl</span>, </span><span class=\"param\"><span class=\"n\">group</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.experimental.demb.triton_sp_embedding_lookup_backward", "modulename": "linghe.experimental.demb", "qualname": "triton_sp_embedding_lookup_backward", "kind": "function", "doc": "<p>inplace update embedding weight gradient</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output</li>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>g_ptr:</strong>  data_ptr of embedding weight gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">grad_output</span>,</span><span class=\"param\">\t<span class=\"n\">input_ids</span>,</span><span class=\"param\">\t<span class=\"n\">g_ptr</span>,</span><span class=\"param\">\t<span class=\"n\">vocab_size</span>,</span><span class=\"param\">\t<span class=\"n\">hdl</span>,</span><span class=\"param\">\t<span class=\"n\">group</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span>,</span><span class=\"param\">\t<span class=\"n\">gathered_input_ids</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.experimental.dla", "modulename": "linghe.experimental.dla", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental.dmm", "modulename": "linghe.experimental.dmm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental.dmm.triton_split_tp_gemm", "modulename": "linghe.experimental.dmm", "qualname": "triton_split_tp_gemm", "kind": "function", "doc": "<p>tensor-parallel fc2 in the shared expert, use split-k implementation\ny = all_reduce(x @ fc2)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  left matrix with bf16 precision</li>\n<li><strong>b:</strong>  right matrix with bf16 precision</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: all-reduced output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">hdl</span>,</span><span class=\"param\">\t<span class=\"n\">group</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">distributed_c10d</span><span class=\"o\">.</span><span class=\"n\">ProcessGroup</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.experimental.gmem_barrier_arrive_wait", "modulename": "linghe.experimental.gmem_barrier_arrive_wait", "kind": "module", "doc": "<p>copied from <a href=\"https://github.com/meta-pytorch/kraken/blob/main/kraken/_ptx_utils/gmem_barrier_arrive_wait.py\">https://github.com/meta-pytorch/kraken/blob/main/kraken/_ptx_utils/gmem_barrier_arrive_wait.py</a></p>\n"}, {"fullname": "linghe.experimental.gmem_barrier_arrive_wait.wait_gmem_barrier", "modulename": "linghe.experimental.gmem_barrier_arrive_wait", "qualname": "wait_gmem_barrier", "kind": "function", "doc": "<p>Wait for a global memory barrier to reach the expected state.</p>\n\n<p>This function implements a spin-wait loop that continuously checks a memory location\nuntil it reaches the expected value, providing synchronization across GPU threads.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>addr:</strong>  Memory address of the barrier to wait on (Must be a scalar)</li>\n<li><strong>expect:</strong>  Expected value to wait for (default: 1)</li>\n<li><strong>update:</strong>  Update the barrier with once acquired (default: 0)</li>\n<li><strong>sem:</strong>  Memory semantics for the atomic operation (default: \"acquire\")</li>\n<li><strong>scope:</strong>  Scope of the atomic operation. Options: \"gpu\", \"sys\" (default: \"gpu\")</li>\n<li><strong>op:</strong>  Atomic operation type (default: \"ld\", currently only supported option)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">addr</span>,</span><span class=\"param\">\t<span class=\"n\">expect</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">update</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">sem</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;acquire&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">scope</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;gpu&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">op</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;ld&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">skip_sync</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.experimental.symm_mem_barrier", "modulename": "linghe.experimental.symm_mem_barrier", "kind": "module", "doc": "<p>copied from <a href=\"https://github.com/meta-pytorch/kraken/blob/main/kraken/_ptx_utils/symm_mem_barrier.py\">https://github.com/meta-pytorch/kraken/blob/main/kraken/_ptx_utils/symm_mem_barrier.py</a></p>\n"}, {"fullname": "linghe.experimental.symm_mem_barrier.symm_mem_sync", "modulename": "linghe.experimental.symm_mem_barrier", "qualname": "symm_mem_sync", "kind": "function", "doc": "<p>Synchronizes blocks with matching block_id across participating devices.</p>\n\n<p>Note: the function itself is not a system level barrier/fence. It is a\nbuilding block for expressing different synchronization patterns.</p>\n\n<p>Pattern 0: Ensures that all writes to symm_mem buffers from previous\nkernels across all devices are visible to the current kernel:</p>\n\n<pre><code>symm_mem_sync(..., hasPreviousMemAccess=False, hasSubsequentMemAccess=True)\n</code></pre>\n\n<p>Pattern 1: Ensures that all writes to symm_mem buffers from the current\nblock are visible to all remote blocks with matching blockIdx:</p>\n\n<pre><code>symm_mem_sync(..., hasPreviousMemAccess=True, hasSubsequentMemAccess=True)\n</code></pre>\n\n<p>Pattern 2: Ensures that symm_mem buffers read by the current kernel are safe\nfor writing by subsequent kernels across all devices.</p>\n\n<pre><code>symm_mem_sync(..., hasPreviousMemAccess=True, hasSubsequentMemAccess=False)\n</code></pre>\n\n<h6 id=\"cuda-graph-friendliness\">CUDA graph friendliness:</h6>\n\n<blockquote>\n  <p>This barrier operates through atomic operations on a zero-filled signal\n  pad, which resets to a zero-filled state after each successful\n  synchronization. This design eliminates the need for incrementing a\n  flag from host.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">signal_pad_ptrs</span>,</span><span class=\"param\">\t<span class=\"n\">block_id</span>,</span><span class=\"param\">\t<span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">world_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hasPreviousMemAccess</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">hasSubsequentMemAccess</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.experimental.test_demb", "modulename": "linghe.experimental.test_demb", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental.test_dla", "modulename": "linghe.experimental.test_dla", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.experimental.test_dmm", "modulename": "linghe.experimental.test_dmm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade", "modulename": "linghe.facade", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.facade.add", "modulename": "linghe.facade.add", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.add.inplace_add", "modulename": "linghe.facade.add", "qualname": "inplace_add", "kind": "function", "doc": "<p>inplace add y to x with mix precise</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  to be updated</li>\n<li><strong>y:</strong>  add to x</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>updated x tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.emb", "modulename": "linghe.facade.emb", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.emb.deprecated_fused_accumulation_embedding_lookup", "modulename": "linghe.facade.emb", "qualname": "deprecated_fused_accumulation_embedding_lookup", "kind": "function", "doc": "<p>embedding lookup</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input ids</li>\n<li><strong>w_ptr:</strong> </li>\n<li><strong>g_ptr:</strong> </li>\n<li><strong>dim:</strong> </li>\n<li><strong>dtype:</strong> </li>\n<li><strong>grad_dtype:</strong> </li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>lookup output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w_ptr</span>, </span><span class=\"param\"><span class=\"n\">g_ptr</span>, </span><span class=\"param\"><span class=\"n\">dim</span>, </span><span class=\"param\"><span class=\"n\">dtype</span>, </span><span class=\"param\"><span class=\"n\">grad_dtype</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.emb.fused_accumulation_embedding_lookup", "modulename": "linghe.facade.emb", "qualname": "fused_accumulation_embedding_lookup", "kind": "function", "doc": "<p>embedding lookup</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input ids</li>\n<li><strong>w:</strong>  embedding weight, should contain a <code>grad_name</code> tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>lookup output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">parameter</span><span class=\"o\">.</span><span class=\"n\">Parameter</span>,</span><span class=\"param\">\t<span class=\"n\">grad_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;grad&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.emb.embedding_lookup", "modulename": "linghe.facade.emb", "qualname": "embedding_lookup", "kind": "function", "doc": "<p>embedding lookup</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input ids</li>\n<li><strong>w:</strong>  embedding weight</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>lookup output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">parameter</span><span class=\"o\">.</span><span class=\"n\">Parameter</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.fp32_gemm", "modulename": "linghe.facade.fp32_gemm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.fp32_gemm.fp32_gemm", "modulename": "linghe.facade.fp32_gemm", "qualname": "fp32_gemm", "kind": "function", "doc": "<p>gemm with bf16/fp16 inputs and float32 output,\ncurrently used in MoE router gemm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input:</strong>  bf16/fp16 activation tensor</li>\n<li><strong>weight:</strong>  bf16/fp16 weight tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output of gemm</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"nb\">input</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.gate", "modulename": "linghe.facade.gate", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.gate.group_rms_norm_gate", "modulename": "linghe.facade.gate", "qualname": "group_rms_norm_gate", "kind": "function", "doc": "<p>return group_rms_norm(transpose(attn_output, [0,1]), weight) * sigmoid(gate)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>attn_output:</strong>  output of core attn, shape [bs, length, n_heads, head_dim]</li>\n<li><strong>gate:</strong>  gate tensor for attention output, shape [length, bs, dim]</li>\n<li><strong>weight:</strong>  weight of RMS norm, shape [dim]</li>\n<li><strong>eps:</strong>  epsilon for RMS</li>\n<li><strong>group_size:</strong>  group size of group RMS norm</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output with shape [length, bs, dim]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">attn_output</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">group_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.hadamard_quant_linear", "modulename": "linghe.facade.hadamard_quant_linear", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.hadamard_quant_linear.HadamardQuantLinear", "modulename": "linghe.facade.hadamard_quant_linear", "qualname": "HadamardQuantLinear", "kind": "class", "doc": "<p>a naive implementation of hadamard transformation and quantization</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "linghe.facade.hadamard_quant_linear.HadamardQuantLinear.__init__", "modulename": "linghe.facade.hadamard_quant_linear", "qualname": "HadamardQuantLinear.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>in_features:</strong>  in feature number</li>\n<li><strong>out_features:</strong>  out feature number</li>\n<li><strong>bias:</strong>  whether use bias</li>\n<li><strong>device:</strong>  weight device</li>\n<li><strong>dtype:</strong>  weight dtype</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "linghe.facade.loss", "modulename": "linghe.facade.loss", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.loss.softmax_cross_entropy", "modulename": "linghe.facade.loss", "qualname": "softmax_cross_entropy", "kind": "function", "doc": "<p>softmax cross entropy</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logits tensor, shape [...,dim]</li>\n<li><strong>labels:</strong>  labels tensor, shape [...]</li>\n<li><strong>inplace:</strong>  update gradient in the <code>logits</code> tensor if True</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a tensor of per token loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_index</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">inplace</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">tp_group</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.loss.moe_z_loss", "modulename": "linghe.facade.loss", "qualname": "moe_z_loss", "kind": "function", "doc": "<p>softmax cross entropy</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logits tensor, shape [...,dim]</li>\n<li><strong>coef:</strong>  z loss coef</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>z loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">logits</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">coef</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.mla", "modulename": "linghe.facade.mla", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.mla.multi_latend_attention", "modulename": "linghe.facade.mla", "qualname": "multi_latend_attention", "kind": "function", "doc": "<p>inplace add y to x with mix precise</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  to be updated</li>\n<li><strong>y:</strong>  add to x</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>updated x tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">q</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">k</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">v</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">padded_cu_seqlens</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">max_q_length</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">causal</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">safe</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">clip_value</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.norm", "modulename": "linghe.facade.norm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.norm.rms_norm", "modulename": "linghe.facade.norm", "qualname": "rms_norm", "kind": "function", "doc": "<p>rms norm of x with weight</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  activation tensor</li>\n<li><strong>weight:</strong>  weight tensor</li>\n<li><strong>eps:</strong>  epsilon for RMS</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>rms output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.norm.BlockRMSNorm", "modulename": "linghe.facade.norm", "qualname": "BlockRMSNorm", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "linghe.facade.norm.BlockRMSNorm.forward", "modulename": "linghe.facade.norm", "qualname": "BlockRMSNorm.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"nb\">input</span>, </span><span class=\"param\"><span class=\"n\">weight</span>, </span><span class=\"param\"><span class=\"n\">rms</span>, </span><span class=\"param\"><span class=\"n\">eps</span>, </span><span class=\"param\"><span class=\"n\">quantizer</span>, </span><span class=\"param\"><span class=\"bp\">cls</span>, </span><span class=\"param\"><span class=\"n\">is_recomputing</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.norm.BlockRMSNorm.backward", "modulename": "linghe.facade.norm", "qualname": "BlockRMSNorm.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">grad_rms</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.permutation", "modulename": "linghe.facade.permutation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.facade.permutation.padded_permute", "modulename": "linghe.facade.permutation", "qualname": "padded_permute", "kind": "function", "doc": "<p>Permute the tokens and probs based on the mask.\nTokens with the same designated expert will be grouped together.\nThe shape of mask is [tokens, num_experts], it indicates which experts were selected\nby each token.\nWhen drop_and_pad=True, in routing_map, the number of non-zeros in each column equals to\nexpert capacity. This function exploits this feature to use ops that support cuda graph.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tokens (torch.Tensor):</strong>  The input token tensor, [num_tokens, hidden].</li>\n<li><strong>routing_map (torch.Tensor):</strong>  The sparse token to expert mapping, [num_tokens, num_experts].</li>\n<li><strong>tokens_per_expert (torch.Tensor):</strong>  cpu tensor</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokens</span>,</span><span class=\"param\">\t<span class=\"n\">routing_map</span>,</span><span class=\"param\">\t<span class=\"n\">tokens_per_expert_cuda_tensor</span>,</span><span class=\"param\">\t<span class=\"n\">tokens_per_expert_list</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.permutation.block_padded_permute", "modulename": "linghe.facade.permutation", "qualname": "block_padded_permute", "kind": "function", "doc": "<p>Permute the tokens and probs based on the mask.\nTokens with the same designated expert will be grouped together.\nThe shape of mask is [tokens, num_experts], it indicates which experts were selected\nby each token.\nWhen drop_and_pad=True, in routing_map, the number of non-zeros in each column equals to\nexpert capacity. This function exploits this feature to use ops that support cuda graph.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>tokens (torch.Tensor):</strong>  The input token tensor, [num_tokens, hidden].</li>\n<li><strong>routing_map (torch.Tensor):</strong>  The sparse token to expert mapping, [num_tokens, num_experts].</li>\n<li><strong>tokens_per_expert (torch.Tensor):</strong>  cpu tensor</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">tokens</span>,</span><span class=\"param\">\t<span class=\"n\">routing_map</span>,</span><span class=\"param\">\t<span class=\"n\">tokens_per_expert_cuda_tensor</span>,</span><span class=\"param\">\t<span class=\"n\">tokens_per_expert_list</span>,</span><span class=\"param\">\t<span class=\"n\">quantizers</span>,</span><span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.rope", "modulename": "linghe.facade.rope", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.rope.qk_norm_half_rope", "modulename": "linghe.facade.rope", "qualname": "qk_norm_half_rope", "kind": "function", "doc": "<p>split qkv to q/k/v, apply qk norm and half rope to q/k, transpose q/k/v to flash-attention layout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>qkv:</strong>  QKV tensor with size of [S, B, dim] or [T, dim] , heads are interleaved</li>\n<li><strong>q_norm_weight:</strong>  rms norm weight for query</li>\n<li><strong>k_norm_weight:</strong>  rms norm weight for key</li>\n<li><strong>freqs:</strong>  Freqs tensor based on half dim.</li>\n<li><strong>cu_seqlens_q:</strong>  accumulated query lengths, [num_seqs + 1]</li>\n<li><strong>cu_seqlens_kv:</strong>  accumulated kv lengths, [num_seqs + 1]</li>\n<li><strong>H:</strong>  Number of attention heads.</li>\n<li><strong>h:</strong>  Number of key/value heads.</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>cp_rank:</strong>  context parallel rank</li>\n<li><strong>cp_size:</strong>  context parallel size</li>\n<li><strong>mscale:</strong>  mscale for rope</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: shape [B, S, H, head_dim] or [T, H, head_dim]</li>\n  <li>ko: shape [B, S, h, head_dim] or [T, h, head_dim]</li>\n  <li>vo: shape [B, S, h, head_dim] or [T, h, head_dim]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">qkv</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">q_norm_weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">k_norm_weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_q</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_kv</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">H</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">h</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">cp_rank</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">cp_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">mscale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">silu</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reuse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.rope.mla_rope", "modulename": "linghe.facade.rope", "qualname": "mla_rope", "kind": "function", "doc": "<p>inplace apply rope to tail 64 dims, split kv and apply rope to k_pos_emb and copy to k</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>q:</strong>  query tensor with size of [S, B, H, 128] (cu_seqlens is None) \nor [N, H, 128] (cu_seqlens is not None)</li>\n<li><strong>kv:</strong>  kv tensor with size of [S, B, H, 256] (cu_seqlens is None) or \n[N, H, 256] (cu_seqlens is not None)</li>\n<li><strong>k_pos_emb:</strong>  k pos emb with size of [S, B, 1, 64] (cu_seqlens is None) or \n[N, 1, 64] (cu_seqlens is not None)</li>\n<li><strong>freqs:</strong>  Freqs tensor with size of [S, 64]</li>\n<li><strong>cu_seqlens_q:</strong>  cumulative query lengths tensor with size of [B+1]</li>\n<li><strong>cu_seqlens_kv:</strong>  cumulative kv lengths tensor with size of [B+1]</li>\n<li><strong>mscale:</strong>  mscale of rope</li>\n<li><strong>transpose:</strong>  whether transpose output layout to [B, S, H, DIM]</li>\n<li><strong>cp_size:</strong>  context-parallel size</li>\n<li><strong>cp_rank:</strong>  context-parallel rank</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: shape [S, B, H, 192] or [N, H, 192]</li>\n  <li>ko: shape [S, B, H, 192] or [N, H, 192]</li>\n  <li>vo: shape [S, B, H, 128] or [N, H, 128]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">q</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">kv</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">k_pos_emb</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_q</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_kv</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mscale</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">transpose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cp_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">cp_rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">reuse</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.silu", "modulename": "linghe.facade.silu", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.facade.silu.BlockSiluFunction", "modulename": "linghe.facade.silu", "qualname": "BlockSiluFunction", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "linghe.facade.silu.BlockSiluFunction.forward", "modulename": "linghe.facade.silu", "qualname": "BlockSiluFunction.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"nb\">input</span>, </span><span class=\"param\"><span class=\"n\">quantizer</span>, </span><span class=\"param\"><span class=\"n\">grad_quantizer</span>, </span><span class=\"param\"><span class=\"bp\">cls</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.silu.BlockSiluFunction.backward", "modulename": "linghe.facade.silu", "qualname": "BlockSiluFunction.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.silu.BlockBatchWeightedSiluFunction", "modulename": "linghe.facade.silu", "qualname": "BlockBatchWeightedSiluFunction", "kind": "class", "doc": "<p>Base class to create custom <code>autograd.Function</code>.</p>\n\n<p>To create a custom <code>autograd.Function</code>, subclass this class and implement\nthe <code>forward()</code> and <code>backward()</code> static methods. Then, to use your custom\nop in the forward pass, call the class method <code>apply</code>. Do not call\n<code>forward()</code> directly.</p>\n\n<p>To ensure correctness and best performance, make sure you are calling the\ncorrect methods on <code>ctx</code> and validating your backward function using\n<code>torch.autograd.gradcheck()</code>.</p>\n\n<p>See :ref:<code>extending-autograd</code> for more details on how to use this class.</p>\n\n<p>Examples::</p>\n\n<pre><code>&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n</code></pre>\n", "bases": "torch.autograd.function.Function"}, {"fullname": "linghe.facade.silu.BlockBatchWeightedSiluFunction.forward", "modulename": "linghe.facade.silu", "qualname": "BlockBatchWeightedSiluFunction.forward", "kind": "function", "doc": "<p>Define the forward of the custom autograd Function.</p>\n\n<p>This function is to be overridden by all subclasses.\nThere are two ways to define forward:</p>\n\n<p>Usage 1 (Combined forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    pass\n</code></pre>\n\n<ul>\n<li>It must accept a context ctx as the first argument, followed by any\nnumber of arguments (tensors or other types).</li>\n<li>See :ref:<code>combining-forward-context</code> for more details</li>\n</ul>\n\n<p>Usage 2 (Separate forward and ctx)::</p>\n\n<pre><code>@staticmethod\ndef forward(*args: Any, **kwargs: Any) -&gt; Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -&gt; None:\n    pass\n</code></pre>\n\n<ul>\n<li>The forward no longer accepts a ctx argument.</li>\n<li>Instead, you must also override the <code>torch.autograd.Function.setup_context()</code>\nstaticmethod to handle setting up the <code>ctx</code> object.\n<code>output</code> is the output of the forward, <code>inputs</code> are a Tuple of inputs\nto the forward.</li>\n<li>See :ref:<code>extending-autograd</code> for more details</li>\n</ul>\n\n<p>The context can be used to store arbitrary data that can be then\nretrieved during the backward pass. Tensors should not be stored\ndirectly on <code>ctx</code> (though this is not currently enforced for\nbackward compatibility). Instead, tensors should be saved either with\n<code>ctx.save_for_backward()</code> if they are intended to be used in\n<code>backward</code> (equivalently, <code>vjp</code>) or <code>ctx.save_for_forward()</code>\nif they are intended to be used for in <code>jvp</code>.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">ctx</span>,</span><span class=\"param\">\t<span class=\"nb\">input</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span>,</span><span class=\"param\">\t<span class=\"n\">counts</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span>,</span><span class=\"param\">\t<span class=\"n\">quantizers</span>,</span><span class=\"param\">\t<span class=\"n\">grad_quantizers</span>,</span><span class=\"param\">\t<span class=\"bp\">cls</span>,</span><span class=\"param\">\t<span class=\"n\">is_recomputing</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.silu.BlockBatchWeightedSiluFunction.backward", "modulename": "linghe.facade.silu", "qualname": "BlockBatchWeightedSiluFunction.backward", "kind": "function", "doc": "<p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>\n\n<p>This function is to be overridden by all subclasses.\n(Defining this function is equivalent to defining the <code>vjp</code> function.)</p>\n\n<p>It must accept a context <code>ctx</code> as the first argument, followed by\nas many outputs as the <code>forward()</code> returned (None will be passed in\nfor non tensor outputs of the forward function),\nand it should return as many tensors, as there were inputs to\n<code>forward()</code>. Each argument is the gradient w.r.t the given output,\nand each returned value should be the gradient w.r.t. the\ncorresponding input. If an input is not a Tensor or is a Tensor not\nrequiring grads, you can just pass None as a gradient for that input.</p>\n\n<p>The context can be used to retrieve tensors saved during the forward\npass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple\nof booleans representing whether each input needs gradient. E.g.,\n<code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the\nfirst input to <code>forward()</code> needs gradient computed w.r.t. the\noutput.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">ctx</span>, </span><span class=\"param\"><span class=\"n\">grad_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.smooth_quant_linear", "modulename": "linghe.facade.smooth_quant_linear", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.smooth_quant_linear.SmoothQuantLinear", "modulename": "linghe.facade.smooth_quant_linear", "qualname": "SmoothQuantLinear", "kind": "class", "doc": "<p>a naive implementation of smooth quantization linear</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "linghe.facade.smooth_quant_linear.SmoothQuantLinear.__init__", "modulename": "linghe.facade.smooth_quant_linear", "qualname": "SmoothQuantLinear.__init__", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>in_features:</strong>  in feature number</li>\n<li><strong>out_features:</strong>  out feature number</li>\n<li><strong>bias:</strong>  whether use bias</li>\n<li><strong>device:</strong>  weight device</li>\n<li><strong>dtype:</strong>  weight dtype</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">out_features</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "linghe.facade.topk", "modulename": "linghe.facade.topk", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.topk.fused_topk", "modulename": "linghe.facade.topk", "qualname": "fused_topk", "kind": "function", "doc": "<p>topk</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>k:</strong>  topk</li>\n<li><strong>dim:</strong>  dimension to apply topk, only support -1 currently</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>values: topk values\n  indices: topk indices</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">k</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.topk.group_topk_score", "modulename": "linghe.facade.topk", "qualname": "group_topk_score", "kind": "function", "doc": "<p>group topk with softmax/sigmoid function</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input logit tensor</li>\n<li><strong>topk:</strong>  topk</li>\n<li><strong>expert_bias:</strong>  expert bias</li>\n<li><strong>num_groups:</strong>  number of groups</li>\n<li><strong>group_topk:</strong>  group to apply topk</li>\n<li><strong>scaling_factor:</strong>  scaling factor</li>\n<li><strong>score_function:</strong>  scaling function</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>probs: topk probs\n  routing_map: topk binary map\n  counts: token count per expert</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">topk</span>,</span><span class=\"param\">\t<span class=\"n\">expert_bias</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_groups</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">group_topk</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">scaling_factor</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">score_function</span><span class=\"o\">=</span><span class=\"s1\">&#39;sigmoid&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.facade.transpose", "modulename": "linghe.facade.transpose", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.facade.transpose.transpose", "modulename": "linghe.facade.transpose", "qualname": "transpose", "kind": "function", "doc": "<p>transpose a tensor, x.ndims should not greater than 4</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>inner:</strong>   if True, transpose the first two dimensions\nif False, transpose the last two dimensions</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a transposed tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">inner</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm", "modulename": "linghe.gemm", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.gemm.blockwise_fp8_gemm", "modulename": "linghe.gemm.blockwise_fp8_gemm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.gemm.channelwise_fp8_gemm", "modulename": "linghe.gemm.channelwise_fp8_gemm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.gemm.channelwise_fp8_gemm.triton_scaled_mm", "modulename": "linghe.gemm.channelwise_fp8_gemm", "qualname": "triton_scaled_mm", "kind": "function", "doc": "<p>similar to torch._scaled_mm, support accumulating gemm output to c\n    and low precision output tensor</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  left fp8 tensor</li>\n<li><strong>b:</strong>  right fp8 tensor, column-major</li>\n<li><strong>a_scale:</strong>  fp32 scale of a</li>\n<li><strong>b_scale:</strong>  fp32 scale of b</li>\n<li><strong>out_dtype:</strong>  output tensor dtype</li>\n<li><strong>c:</strong>  output tensor</li>\n<li><strong>accum:</strong>  accumulate output on c if True</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">a_scale</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">b_scale</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">out_dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span>,</span><span class=\"param\">\t<span class=\"n\">c</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">accum</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm", "modulename": "linghe.gemm.fp32_gemm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.gemm.fp32_gemm.triton_fp32_gemm", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_fp32_gemm", "kind": "function", "doc": "<p>return fp32 gemm result with fp16/bf16 inputs,\n    it's mainly used for MoE router GEMM\n    and DO NOT suitable for large size GEMM</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  left matrix with fp16/bf16 precision</li>\n<li><strong>b:</strong>  right matrix with fp16/bf16 precision</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: output with fp32 precision</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm.triton_fp32_gemm_for_backward", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_fp32_gemm_for_backward", "kind": "function", "doc": "<p>mix precision gemm for backward, a@b.float()</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  input gradient, fp32</li>\n<li><strong>b:</strong>  gemm weight, bf16/fp16</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: gradient of activation</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm.triton_fp32_gemm_for_update", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_fp32_gemm_for_update", "kind": "function", "doc": "<p>mix precision gemm for updaing weight</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output, fp32</li>\n<li><strong>x:</strong>  input activation, bf16/fp16</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: gradient of weight</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm.triton_split_fp32_gemm", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_split_fp32_gemm", "kind": "function", "doc": "<p>return fp32 gemm result with fp16/bf16 inputs,\n    it's mainly used for MoE router GEMM\n    and DO NOT suitable for large size GEMM</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  left matrix with fp16/bf16 precision</li>\n<li><strong>b:</strong>  right matrix with fp16/bf16 precision</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: output with fp32 precision</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm.triton_split_fp32_gemm_for_backward", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_split_fp32_gemm_for_backward", "kind": "function", "doc": "<p>mix precision gemm for backward, a@b.float()</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>a:</strong>  input gradient, fp32</li>\n<li><strong>b:</strong>  gemm weight, bf16/fp16</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: gradient of activation</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.gemm.fp32_gemm.triton_split_fp32_gemm_for_update", "modulename": "linghe.gemm.fp32_gemm", "qualname": "triton_split_fp32_gemm_for_update", "kind": "function", "doc": "<p>mix precision gemm for updaing weight</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output, fp32</li>\n<li><strong>x:</strong>  input activation, bf16/fp16</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>c: gradient of weight</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant", "modulename": "linghe.quant", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.quant.block", "modulename": "linghe.quant.block", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.quant.block.triton_block_quant", "modulename": "linghe.quant.block", "qualname": "triton_block_quant", "kind": "function", "doc": "<p>blockwise quantize x, used for blockwise recipe for weight in megatron</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>block_size:</strong>  block wise</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>y: quantized tensor, float8_e4m3fn</li>\n  <li>s: quantization scale, float32</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">block_size</span><span class=\"o\">=</span><span class=\"mi\">128</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.block.triton_blockwise_quant", "modulename": "linghe.quant.block", "qualname": "triton_blockwise_quant", "kind": "function", "doc": "<p>blockwise quantization, used in blockwise recipt in megatron</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n<li><strong>output_mode:</strong>  one of {0, 1, 2}\n0: only output non-transposed quantized tensor\n1: only output transposed quantized tensor\n2: output both</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_q: \n  x_scale: \n  xt_q: \n  xt_scale:</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">output_mode</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.block.triton_batch_blockwise_quant", "modulename": "linghe.quant.block", "qualname": "triton_batch_blockwise_quant", "kind": "function", "doc": "<p>select and quant, used in megatron 0.12 flex moe</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  [bs, dim]</li>\n<li><strong>token_count_per_expert:</strong>  [n_experts]</li>\n<li><strong>splits:</strong>  python int list of token_count_per_expert</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>x_q: </li>\n  <li>x_scale: </li>\n  <li>xt_q: </li>\n  <li>xt_scale:</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"n\">token_count_per_expert</span>, </span><span class=\"param\"><span class=\"n\">splits</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.channel", "modulename": "linghe.quant.channel", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.quant.channel.triton_row_quant", "modulename": "linghe.quant.channel", "qualname": "triton_row_quant", "kind": "function", "doc": "<p>rowwise quantize x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input x</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_q: quantized tensor\n  x_scale: quantization scale</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.channel.triton_tokenwise_row_quant", "modulename": "linghe.quant.channel", "qualname": "triton_tokenwise_row_quant", "kind": "function", "doc": "<p>rowwise quantize x with power of 2 dim size</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input x</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out: quantized tensor\n  scale: quantization scale</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.channel.triton_transpose_row_quant", "modulename": "linghe.quant.channel", "qualname": "triton_transpose_row_quant", "kind": "function", "doc": "<p>transpose x and row quantize x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input x</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>x_q: quantized tensor</li>\n  <li>x_scale: quantization scale</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.group", "modulename": "linghe.quant.group", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.quant.group.triton_group_quant", "modulename": "linghe.quant.group", "qualname": "triton_group_quant", "kind": "function", "doc": "<p>groupwise quantize x, group is in under rowwise format</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>group_size:</strong>  group wise</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>y: quantized tensor, float8_e4m3fn</li>\n  <li>s: quantization scale, float32</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float8_e4m3fn</span>, </span><span class=\"param\"><span class=\"n\">group_size</span><span class=\"o\">=</span><span class=\"mi\">128</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.hadamard", "modulename": "linghe.quant.hadamard", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.quant.hadamard.triton_hadamard_quant", "modulename": "linghe.quant.hadamard", "qualname": "triton_hadamard_quant", "kind": "function", "doc": "<p>apply hadamard transformation and then quantize transformed tensor</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>hm:</strong>  hamadard matrix</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>x_q: rowwise quantized tensor of non-transposed x</li>\n  <li>x_scale: rowwise quantization scale of non-transposed x</li>\n  <li>xt_q: columnwise quantized tensor of transposed x</li>\n  <li>xt_scale: columnwise quantization scale of transposed x</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">hm</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.quant.smooth", "modulename": "linghe.quant.smooth", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.tools", "modulename": "linghe.tools", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.tools.benchmark", "modulename": "linghe.tools.benchmark", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.tools.check", "modulename": "linghe.tools.check", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.tools.util", "modulename": "linghe.tools.util", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils", "modulename": "linghe.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.utils.add", "modulename": "linghe.utils.add", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.add.triton_inplace_add", "modulename": "linghe.utils.add", "qualname": "triton_inplace_add", "kind": "function", "doc": "<p>inplace add y to x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  Tensor</li>\n<li><strong>y:</strong>  Tensor</li>\n<li><strong>accum:</strong>  x += y if accum=True else x.copy_(y)</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>updated x</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">y</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">accum</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.emb", "modulename": "linghe.utils.emb", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.emb.triton_embedding_forward", "modulename": "linghe.utils.emb", "qualname": "triton_embedding_forward", "kind": "function", "doc": "<p>inplace add y to x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>w_ptr:</strong>  data_ptr of embedding weight</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>embedding output</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">w_ptr</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">4096</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.emb.triton_atomic_embedding_backward", "modulename": "linghe.utils.emb", "qualname": "triton_atomic_embedding_backward", "kind": "function", "doc": "<p>inplace update embedding weight gradient</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output</li>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>g_ptr:</strong>  data_ptr of embedding weight gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">g_ptr</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.emb.triton_sync_embedding_backward", "modulename": "linghe.utils.emb", "qualname": "triton_sync_embedding_backward", "kind": "function", "doc": "<p>inplace update embedding weight gradient</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output</li>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>g_ptr:</strong>  data_ptr of embedding weight gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">g_ptr</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.emb.triton_embedding_backward", "modulename": "linghe.utils.emb", "qualname": "triton_embedding_backward", "kind": "function", "doc": "<p>inplace update embedding weight gradient</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>y:</strong>  gradient of output</li>\n<li><strong>x:</strong>  input ids Tensor</li>\n<li><strong>g_ptr:</strong>  data_ptr of embedding weight gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>None</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">g_ptr</span>, </span><span class=\"param\"><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">bfloat16</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gate", "modulename": "linghe.utils.gate", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "linghe.utils.gate.triton_group_rms_norm_gate_forward", "modulename": "linghe.utils.gate", "qualname": "triton_group_rms_norm_gate_forward", "kind": "function", "doc": "<p>norm and gate in linear attention</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  output of attn, [bs, length, n_heads, head_dim]</li>\n<li><strong>gate:</strong>  gate tensor, [length, bs, dim] if transpose=True else [bs, length, dim]</li>\n<li><strong>weight:</strong>  rms norm weight, [dim]</li>\n<li><strong>eps:</strong>  epsilon of rms norm</li>\n<li><strong>group_size:</strong>  group size of group rms norm</li>\n<li><strong>transpose:</strong>  whether gate tensor has been transposed and output will be transposed</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output tensor, [length, bs, dim] if transpose=True else [bs, length, dim]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">gate</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">group_size</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">transpose</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather", "modulename": "linghe.utils.gather", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.gather.triton_make_row_id_map", "modulename": "linghe.utils.gather", "qualname": "triton_make_row_id_map", "kind": "function", "doc": "<p>make row id map, values in the tensor are the row indices</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>routing_map:</strong>  a tensor of 0/1 values, 1 indicates routed</li>\n<li><strong>multiple_of:</strong>  padding the tokens of each expert to multiple of this value</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>row id map with shape [n_tokens, n_experts]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">routing_map</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">multiple_of</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_make_row_id_map_and_index", "modulename": "linghe.utils.gather", "qualname": "triton_make_row_id_map_and_index", "kind": "function", "doc": "<p>similar with triton_make_row_id_map, but output an indices tensor as well</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>routing_map:</strong>  [n_tokens, n_experts]</li>\n<li><strong>num_out_tokens:</strong>  sum(round_up_to(n_tokens, multiple_of))</li>\n<li><strong>multiple_of:</strong>  padding the tokens of each expert to this value</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>row_in_map: [n_tokens, n_experts]\n  row_indices: [num_out_tokens]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">routing_map</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">num_out_tokens</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">multiple_of</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_index_select", "modulename": "linghe.utils.gather", "qualname": "triton_index_select", "kind": "function", "doc": "<p>index select for quantized tensor</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  [bs, dim]</li>\n<li><strong>indices:</strong>  [K]</li>\n<li><strong>scale:</strong>  [bs]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out: output of selected x\n  scale_out: scale of selected scale</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">indices</span>, </span><span class=\"param\"><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">scale_out</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_permute_with_mask_map", "modulename": "linghe.utils.gather", "qualname": "triton_permute_with_mask_map", "kind": "function", "doc": "<p>gather quantized tensor with row id map</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inp:</strong>  [num_tokens, hidden_size], rowwise quantized tensor</li>\n<li><strong>scale:</strong>  optional, [num_tokens], quantization scale</li>\n<li><strong>probs:</strong>  optional, router prob, used as weight</li>\n<li><strong>row_id_map:</strong>  [n_experts, num_tokens]\nindex &gt;= 0: row index of output tensor\nindex == -1: ignore\nNote: index may not be contiguous</li>\n<li><strong>num_out_tokens:</strong>  output token count, including padding tokens</li>\n<li><strong>contiguous:</strong>  whether indices in row_id_map is contiguous,\nFalse means padded</li>\n<li><strong>tokens_per_expert:</strong>  [num_experts], token count per expert,\nnon-blocking cuda tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output: permuted quantized tensor\n  permuted_scale: permuted quantization scale\n  permuted_probs: permuted router prob</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inp</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">scale</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">row_id_map</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_out_tokens</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">contiguous</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">tokens_per_expert</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_batch_transpose_smooth_permute_with_indices", "modulename": "linghe.utils.gather", "qualname": "triton_batch_transpose_smooth_permute_with_indices", "kind": "function", "doc": "<p>used for smooth quantization backward in megatron 0.12,\nx is gathered, requantized, padded to multiple of 32 and tranposed</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  dy, [bs, dim], it is smooth quantized</li>\n<li><strong>scale:</strong>  [bs], quantized scale</li>\n<li><strong>org_smooth_scale:</strong>  [dim]</li>\n<li><strong>smooth_scales:</strong>  [n_experts, dim]</li>\n<li><strong>indices:</strong>  [sum(tokens_per_experts)]</li>\n<li><strong>token_count_per_expert:</strong>  [n_experts], tensor of token count per expert</li>\n<li><strong>splits:</strong>  [n_experts], list of token_count_per_expert</li>\n<li><strong>round_scale:</strong>  round quantization scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_q: [sum(roundup(tokens_per_experts)) * dim]\n  x_scale: [sum(roundup(tokens_per_experts))]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">scale</span>,</span><span class=\"param\">\t<span class=\"n\">org_smooth_scale</span>,</span><span class=\"param\">\t<span class=\"n\">smooth_scales</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span>,</span><span class=\"param\">\t<span class=\"n\">token_count_per_expert</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span>,</span><span class=\"param\">\t<span class=\"n\">x_q</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">x_scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_smooth_weighted_permute_with_indices", "modulename": "linghe.utils.gather", "qualname": "triton_smooth_weighted_permute_with_indices", "kind": "function", "doc": "<p>select and smooth and quant, used in megatron 0.11 all2all moe</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grads:</strong>  [bs, dim]</li>\n<li><strong>tokens:</strong>  [bs, dim]</li>\n<li><strong>smooth_scales:</strong>  [n_experts, dim]</li>\n<li><strong>token_count_per_expert:</strong>  [n_experts]</li>\n<li><strong>indices:</strong>  [n_experts*topk]</li>\n<li><strong>reverse:</strong>  whether scale is 1/scale</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_q: [bs*topk, dim]\n  x_scale: [bs<em>topk]\n  x_sum: [bs</em>topk]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">grads</span>,</span><span class=\"param\">\t<span class=\"n\">tokens</span>,</span><span class=\"param\">\t<span class=\"n\">smooth_scales</span>,</span><span class=\"param\">\t<span class=\"n\">token_count_per_expert</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span>,</span><span class=\"param\">\t<span class=\"n\">x_q</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">x_scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">x_sum</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_smooth_permute_with_indices", "modulename": "linghe.utils.gather", "qualname": "triton_smooth_permute_with_indices", "kind": "function", "doc": "<p>select and smooth and quant</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grad_data:</strong>  [bs, dim]</li>\n<li><strong>grad_scale:</strong>  [bs]</li>\n<li><strong>smooth_scales:</strong>  [n_experts, dim]</li>\n<li><strong>token_count_per_expert:</strong>  [n_experts]</li>\n<li><strong>indices:</strong>  [n_experts*topk]</li>\n<li><strong>x_q:</strong>  [bs*topk, dim]</li>\n<li><strong>x_scale:</strong>  [bs*topk]</li>\n<li><strong>reverse:</strong> </li>\n<li><strong>round_scale:</strong> </li>\n</ul>\n\n<p>Returns:</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">grad_data</span>,</span><span class=\"param\">\t<span class=\"n\">grad_scale</span>,</span><span class=\"param\">\t<span class=\"n\">smooth_scales</span>,</span><span class=\"param\">\t<span class=\"n\">token_count_per_expert</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span>,</span><span class=\"param\">\t<span class=\"n\">x_q</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">x_scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_smooth_permute_with_mask_map", "modulename": "linghe.utils.gather", "qualname": "triton_smooth_permute_with_mask_map", "kind": "function", "doc": "<p>gather ( and optional dequant) and smooth quant</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>inp:</strong>  [num_tokens, hidden_size], rowwise quantized tensor</li>\n<li><strong>row_id_map:</strong>  [n_experts, num_tokens], indices</li>\n<li><strong>scale:</strong>  [num_tokens, hs], rowwise_scale_inv, optional</li>\n<li><strong>num_tokens:</strong>  [n_experts]</li>\n<li><strong>num_experts:</strong> </li>\n<li><strong>num_out_tokens:</strong> </li>\n<li><strong>hidden_size:</strong> </li>\n<li><strong>smooth_scales:</strong>  [n_experts, hidden_size]</li>\n<li><strong>reverse:</strong> </li>\n<li><strong>round_scale:</strong> </li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>output: output tensor</li>\n  <li>permuted_scale: permuted scale if scale is not None</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">inp</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">row_id_map</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">scale</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">num_tokens</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_experts</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_out_tokens</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">smooth_scales</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.gather.triton_batch_block_pad_permute_with_indices", "modulename": "linghe.utils.gather", "qualname": "triton_batch_block_pad_permute_with_indices", "kind": "function", "doc": "<p>select and quant, used in megatron 0.12 flex moe</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  [bs, dim]</li>\n<li><strong>token_count_per_expert:</strong>  [n_experts]</li>\n<li><strong>indices:</strong>  [n_experts*topk]</li>\n<li><strong>splits:</strong>  python int list of token_count_per_expert</li>\n<li><strong>probs:</strong>  route weights, [bs, n_experts]</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_q: \n  x_scale: \n  xt_q: \n  xt_scale: \n  prob_output:</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">xs</span>,</span><span class=\"param\">\t<span class=\"n\">token_count_per_expert</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span>,</span><span class=\"param\">\t<span class=\"n\">probs</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss", "modulename": "linghe.utils.loss", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.loss.triton_softmax_cross_entropy_forward", "modulename": "linghe.utils.loss", "qualname": "triton_softmax_cross_entropy_forward", "kind": "function", "doc": "<p>compute token-wise softmax cross entropy loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logits tensor</li>\n<li><strong>labels:</strong>  labels tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss of each token</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">logits</span>, </span><span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">ignore_index</span><span class=\"o\">=-</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss.triton_softmax_cross_entropy_backward", "modulename": "linghe.utils.loss", "qualname": "triton_softmax_cross_entropy_backward", "kind": "function", "doc": "<p>backward of softmax cross entropy loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logit tensor, [bs, dim]</li>\n<li><strong>labels:</strong>  label tensor, [bs]</li>\n<li><strong>sum_exp:</strong>   [bs]</li>\n<li><strong>max_logit:</strong>  [bs]</li>\n<li><strong>output_grad:</strong>  gradient, [bs, dim]</li>\n<li><strong>inplace:</strong>  whether to reuse logits as gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>grad of input: [bs, dim]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">logits</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">sum_exp</span>,</span><span class=\"param\">\t<span class=\"n\">max_logit</span>,</span><span class=\"param\">\t<span class=\"n\">output_grad</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_index</span><span class=\"o\">=-</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss.triton_parallel_softmax_cross_entropy_forward", "modulename": "linghe.utils.loss", "qualname": "triton_parallel_softmax_cross_entropy_forward", "kind": "function", "doc": "<p>compute token-wise softmax cross entropy loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logits tensor</li>\n<li><strong>labels:</strong>  labels tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>loss of each token</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">logits</span>, </span><span class=\"param\"><span class=\"n\">labels</span>, </span><span class=\"param\"><span class=\"n\">group</span>, </span><span class=\"param\"><span class=\"n\">ignore_index</span><span class=\"o\">=-</span><span class=\"mi\">100</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss.triton_parallel_softmax_cross_entropy_backward", "modulename": "linghe.utils.loss", "qualname": "triton_parallel_softmax_cross_entropy_backward", "kind": "function", "doc": "<p>backward of softmax cross entropy loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logit tensor, [bs, dim]</li>\n<li><strong>labels:</strong>  label tensor, [bs]</li>\n<li><strong>sum_exp:</strong>   [bs]</li>\n<li><strong>max_logit:</strong>  [bs]</li>\n<li><strong>output_grad:</strong>  gradient, [bs, dim]</li>\n<li><strong>inplace:</strong>  whether to reuse logits as gradient</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>grad of input: [bs, dim]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">logits</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span>,</span><span class=\"param\">\t<span class=\"n\">sum_exp</span>,</span><span class=\"param\">\t<span class=\"n\">max_logit</span>,</span><span class=\"param\">\t<span class=\"n\">output_grad</span>,</span><span class=\"param\">\t<span class=\"n\">group</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_index</span><span class=\"o\">=-</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss.triton_moe_z_loss_forward", "modulename": "linghe.utils.loss", "qualname": "triton_moe_z_loss_forward", "kind": "function", "doc": "<p>compute moe z loss,\nz_loss = torch.mean(torch.square(torch.logsumexp(logits, dim=-1))) * coef</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>logits:</strong>  logits tensor</li>\n<li><strong>coef:</strong>  z loss coef</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>z loss</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">logits</span>, </span><span class=\"param\"><span class=\"n\">coef</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.loss.triton_moe_z_loss_backward", "modulename": "linghe.utils.loss", "qualname": "triton_moe_z_loss_backward", "kind": "function", "doc": "<p>backward of moe z loss</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grads:</strong>  grad scalar tensor</li>\n<li><strong>logits:</strong>  logit tensor, [L, B, dim]</li>\n<li><strong>coef:</strong>  python scalar</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output_grad: [L, B, dim]</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grads</span>, </span><span class=\"param\"><span class=\"n\">logits</span>, </span><span class=\"param\"><span class=\"n\">coef</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.mul", "modulename": "linghe.utils.mul", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.mul.triton_dot", "modulename": "linghe.utils.mul", "qualname": "triton_dot", "kind": "function", "doc": "<p>vector dot multiply, output = sum(x*y, 1),\nit is used to calculate gradient of router weight</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong> </li>\n<li><strong>y:</strong> </li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output of sum(x*y, 1)</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.mul.triton_inplace_scale", "modulename": "linghe.utils.mul", "qualname": "triton_inplace_scale", "kind": "function", "doc": "<p>inplace scale a tensor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  Tensor.</li>\n<li><strong>scale:</strong>  a python float scale</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">scale</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.mul.triton_batch_scale", "modulename": "linghe.utils.mul", "qualname": "triton_batch_scale", "kind": "function", "doc": "<p>return [x*scale for x in xs],\nused to scale gradient.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  Tensor lists.</li>\n<li><strong>scale:</strong>  a python float scale</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>xs</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"n\">scale</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.norm", "modulename": "linghe.utils.norm", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.norm.triton_rms_norm_forward", "modulename": "linghe.utils.norm", "qualname": "triton_rms_norm_forward", "kind": "function", "doc": "<p>rms norm</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>weight:</strong>  weight of rms norm</li>\n<li><strong>eps:</strong>  epsilon of rms norm</li>\n<li><strong>rms:</strong>  use x*rms to calculate output if rms is not None, \nit will accelerate recompute of rms norm</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out: output tensor\n  rms: 1/rms of input tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">weight</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">rms</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.norm.triton_rms_norm_and_block_quant_forward", "modulename": "linghe.utils.norm", "qualname": "triton_rms_norm_and_block_quant_forward", "kind": "function", "doc": "<p>Fused RMSNorm forward and block quantization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  Input tensor, shape [M, N]</li>\n<li><strong>weight:</strong>  RMSNorm weight,  shape [N]</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>out:</strong>  output of quantization data</li>\n<li><strong>scale:</strong>  output of quantization scale.</li>\n<li><strong>rms:</strong>  output of rms</li>\n<li><strong>round_scale:</strong>  Set whether to force power of 2 scales.</li>\n<li><strong>output_mode:</strong>  one of {0, 1, 2}.\n0: only output non-transpose tensor\n1: only output transposed tensor\n2: return both</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>out: quantization data.</li>\n  <li>scale: quantization scale.</li>\n  <li>rms: Reciprocal of the root mean square of the\n  input calculated over the last dimension.</li>\n  <li>transpose_output: quantization data of transposed gradient.</li>\n  <li>transpose_scale: quantization scale of transposed gradient.</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">out</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scale</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rms</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.norm.triton_rms_norm_fp32_gemm_block_quant_forward", "modulename": "linghe.utils.norm", "qualname": "triton_rms_norm_fp32_gemm_block_quant_forward", "kind": "function", "doc": "<p>y = rms_norm(x)\nlogits = y@w_route\nx_q, x_s, xt_q, xt_s = quantization(y)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>norm weight:</strong>  weight tensor of rms norm</li>\n<li><strong>route_weight:</strong>  moe router weight</li>\n<li><strong>eps:</strong>  epsilon of rms norm</li>\n<li><strong>output_mode:</strong>  0 or 1\n0: only output non-transpose quantizatino tensor\n1: only output transposed quantizatino tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>y: rms normed tensor</li>\n  <li>rms: 1/rms</li>\n  <li>logits: router logit</li>\n  <li>x_q: </li>\n  <li>x_s: </li>\n  <li>xt_q:</li>\n  <li>xt_s:</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">norm_weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">route_weight</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">rms</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rearange", "modulename": "linghe.utils.rearange", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.rearange.triton_sort_chunks_by_index", "modulename": "linghe.utils.rearange", "qualname": "triton_sort_chunks_by_index", "kind": "function", "doc": "<p>split x to multiple tensors and cat with indices,\nit is used for permutation in moe with all2all communication</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  [bs, dim]</li>\n<li><strong>counts:</strong>  [n_split]</li>\n<li><strong>indices:</strong>  [n_split]</li>\n<li><strong>scales:</strong>  [bs]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>y: output tensor</li>\n  <li>output_scales: output scales if scales is not None</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">counts</span>, </span><span class=\"param\"><span class=\"n\">indices</span>, </span><span class=\"param\"><span class=\"n\">scales</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.reduce", "modulename": "linghe.utils.reduce", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.reduce.triton_abs_max", "modulename": "linghe.utils.reduce", "qualname": "triton_abs_max", "kind": "function", "doc": "<p>columnwise abs max of x, it is used in smooth quantization</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor, may be quantized tensor</li>\n<li><strong>scale:</strong>  quantization scale if x is quantized</li>\n<li><strong>smooth_scale:</strong>  optional smooth scale</li>\n<li><strong>min_value:</strong>  output = max(max(abs(x,0)), min_value)</li>\n<li><strong>axis:</strong>  reduce axis</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>max tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">smooth_scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">min_value</span><span class=\"o\">=</span><span class=\"mf\">1e-30</span>, </span><span class=\"param\"><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.reduce.triton_batch_count_zero", "modulename": "linghe.utils.reduce", "qualname": "triton_batch_count_zero", "kind": "function", "doc": "<p>count zero in tensor list, it is used to monitor zeros in gradient tensor</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  input tensors</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a single-value int64 tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.reduce.triton_norm", "modulename": "linghe.utils.reduce", "qualname": "triton_norm", "kind": "function", "doc": "<p>calculate norm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor.</li>\n<li><strong>ord:</strong>  the order of tensor. -1 means 'inf' ord.</li>\n<li><strong>norm:</strong>  only used with ord in (1, 2)\nTrue: (sum(sum(abs(x)<strong>ord) x for x in xs))</strong>(1/ord) \nFalse: sum(sum(abs(x)**ord) x for x in xs))</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a scalar if scalar=True else a single-value fp32 tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"nb\">ord</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">scalar</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.reduce.triton_batch_norm", "modulename": "linghe.utils.reduce", "qualname": "triton_batch_norm", "kind": "function", "doc": "<p>treat multiple tensors as a single tensor and calculate norm.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  Tensor lists.</li>\n<li><strong>ord:</strong>  the order of tensor. -1 means 'inf' ord.</li>\n<li><strong>norm:</strong>  only used with ord in (1, 2)\nTrue: (sum(sum(abs(x)<strong>ord) x for x in xs))</strong>(1/ord) \nFalse: sum(sum(abs(x)**ord) x for x in xs))</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>a scalar if scalar=True else a single-value fp32 tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"nb\">ord</span><span class=\"o\">=</span><span class=\"mi\">2</span>, </span><span class=\"param\"><span class=\"n\">norm</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">scalar</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">high_precision</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope", "modulename": "linghe.utils.rope", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.rope.triton_half_rope_forward", "modulename": "linghe.utils.rope", "qualname": "triton_half_rope_forward", "kind": "function", "doc": "<p>apply half rope to qk</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>q:</strong>  query tensor, [len, bs, q_head, head_dim]</li>\n<li><strong>k:</strong>  key tensor, [len, bs, kv_head, head_dim]</li>\n<li><strong>freqs:</strong>  rope freqs</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: query output</li>\n  <li>ko: key output</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">q</span>, </span><span class=\"param\"><span class=\"n\">k</span>, </span><span class=\"param\"><span class=\"n\">freqs</span>, </span><span class=\"param\"><span class=\"n\">transposed</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope.triton_qk_norm_and_half_rope_forward", "modulename": "linghe.utils.rope", "qualname": "triton_qk_norm_and_half_rope_forward", "kind": "function", "doc": "<p>split qkv to q/k/v, apply qk norm and half rope to q/k,\n    transpose q/k/v to flash-attention layout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>qkv:</strong>  QKV tensor with size of [S, B, dim], heads are interleaved</li>\n<li><strong>q_norm_weight:</strong>  rms norm weight for query</li>\n<li><strong>k_norm_weight:</strong>  rms norm weight for key</li>\n<li><strong>freqs:</strong>  Freqs tensor based on half dim.</li>\n<li><strong>H:</strong>  Number of attention heads.</li>\n<li><strong>h:</strong>  Number of key/value heads.</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>interleaved:</strong>  whether head of qkv is interleaved,\ninterleaved: [q...qkvq...qkv]\nnon-interleaved: [q...qk...kv...v]</li>\n<li><strong>transposed:</strong>  whether qkv is tranposed\ntransposed: [S, B, dim]\nnon-transposed: [B, S, dim]</li>\n<li><strong>silu:</strong>  apply silu on qkv before qk norm and rope</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: shape [B, S, H, head_dim]</li>\n  <li>ko: shape [B, S, h, head_dim]</li>\n  <li>vo: shape [B, S, h, head_dim]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">qkv</span>,</span><span class=\"param\">\t<span class=\"n\">q_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">k_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">H</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">h</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">interleaved</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transposed</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">silu</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope.triton_qk_norm_and_half_rope_backward", "modulename": "linghe.utils.rope", "qualname": "triton_qk_norm_and_half_rope_backward", "kind": "function", "doc": "<p>backward kernel of triton_qk_norm_and_half_rope_forward</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gq:</strong>  gradient of qo, [len, bs, q_head, head_dim]</li>\n<li><strong>gk:</strong>  gradient of ko, [len, bs, q_head, head_dim]</li>\n<li><strong>gv:</strong>  gradient of vo, [len, bs, q_head, head_dim]</li>\n<li><strong>qkv:</strong>  input qkv</li>\n<li><strong>q_norm_weight:</strong>  rms norm weight for query</li>\n<li><strong>k_norm_weight:</strong>  rms norm weight for key</li>\n<li><strong>freqs:</strong>  Freqs tensor based on half dim.</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>interleaved:</strong>  whether head of qkv is interleaved,\ninterleaved: [q...qkvq...qkv]\nnon-interleaved: [q...qk...kv...v]</li>\n<li><strong>transposed:</strong>  whether qkv is tranposed\ntransposed: [S, B, dim]\nnon-transposed: [B, S, dim]</li>\n<li><strong>silu:</strong>  whether silu is applied to qkv</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>dqkv: gradient of qkv</li>\n  <li>dqw: gradient of q_norm_weight</li>\n  <li>dkw: gradient of k_norm_weight</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gq</span>,</span><span class=\"param\">\t<span class=\"n\">gk</span>,</span><span class=\"param\">\t<span class=\"n\">gv</span>,</span><span class=\"param\">\t<span class=\"n\">qkv</span>,</span><span class=\"param\">\t<span class=\"n\">q_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">k_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">interleaved</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">transposed</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">silu</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope.triton_varlen_qk_norm_and_half_rope_forward", "modulename": "linghe.utils.rope", "qualname": "triton_varlen_qk_norm_and_half_rope_forward", "kind": "function", "doc": "<p>split qkv to q/k/v, apply qk norm and half rope to q/k,\n    transpose q/k/v to flash-attention layout</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>qkv:</strong>  QKV tensor with size of [S, B, dim], heads are interleaved</li>\n<li><strong>q_norm_weight:</strong>  rms norm weight for query</li>\n<li><strong>k_norm_weight:</strong>  rms norm weight for key</li>\n<li><strong>freqs:</strong>  Freqs tensor based on half dim.</li>\n<li><strong>H:</strong>  Number of attention heads.</li>\n<li><strong>h:</strong>  Number of key/value heads.</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>interleaved:</strong>  whether head of qkv is interleaved,\ninterleaved: [q...qkvq...qkv]\nnon-interleaved: [q...qk...kv...v]</li>\n<li><strong>silu:</strong>  apply silu on qkv before qk norm and rope</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: shape [B, S, H, head_dim]</li>\n  <li>ko: shape [B, S, h, head_dim]</li>\n  <li>vo: shape [B, S, h, head_dim]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">qkv</span>,</span><span class=\"param\">\t<span class=\"n\">q_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">k_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_q</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_kv</span>,</span><span class=\"param\">\t<span class=\"n\">H</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">h</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">interleaved</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">silu</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cp_rank</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">cp_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">mscale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">reuse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope.triton_varlen_qk_norm_and_half_rope_backward", "modulename": "linghe.utils.rope", "qualname": "triton_varlen_qk_norm_and_half_rope_backward", "kind": "function", "doc": "<p>backward kernel of triton_qk_norm_and_half_rope_forward</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gq:</strong>  gradient of qo, [len, bs, q_head, head_dim]</li>\n<li><strong>gk:</strong>  gradient of ko, [len, bs, q_head, head_dim]</li>\n<li><strong>gv:</strong>  gradient of vo, [len, bs, q_head, head_dim]</li>\n<li><strong>qkv:</strong>  input qkv</li>\n<li><strong>q_norm_weight:</strong>  rms norm weight for query</li>\n<li><strong>k_norm_weight:</strong>  rms norm weight for key</li>\n<li><strong>freqs:</strong>  Freqs tensor based on half dim.</li>\n<li><strong>eps:</strong>  epsilon value for L2 normalization.</li>\n<li><strong>interleaved:</strong>  whether head of qkv is interleaved,\ninterleaved: [q...qkvq...qkv]\nnon-interleaved: [q...qk...kv...v]</li>\n<li><strong>silu:</strong>  whether silu is applied to qkv</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>dqkv: gradient of qkv</li>\n  <li>dqw: gradient of q_norm_weight</li>\n  <li>dkw: gradient of k_norm_weight</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gq</span>,</span><span class=\"param\">\t<span class=\"n\">gk</span>,</span><span class=\"param\">\t<span class=\"n\">gv</span>,</span><span class=\"param\">\t<span class=\"n\">qkv</span>,</span><span class=\"param\">\t<span class=\"n\">q_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">k_norm_weight</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_q</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_kv</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-06</span>,</span><span class=\"param\">\t<span class=\"n\">interleaved</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">silu</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cp_rank</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">cp_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">mscale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">reuse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.rope.triton_mla_rope_forward", "modulename": "linghe.utils.rope", "qualname": "triton_mla_rope_forward", "kind": "function", "doc": "<p>apply MLA-type rope to qkv</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>q:</strong>  query tensor, [len, bs, n_heads, 192]</li>\n<li><strong>kv:</strong>  key-value tensor, [len, bs, n_heads, 256]</li>\n<li><strong>k_pos_emb:</strong>  k pos emb, [len, bs, 1, 64]</li>\n<li><strong>freqs:</strong>  rope freqs, [len, 64]</li>\n<li><strong>mscale:</strong>  mscale for rope</li>\n<li><strong>transpose:</strong>  whether transpose the output to [bs, len, n_heads, dim] layout</li>\n<li><strong>cu_seqlens_q:</strong>  accummulated query length</li>\n<li><strong>cu_seqlens_kv:</strong>  accummulated kv length</li>\n<li><strong>cp_rank:</strong>  rank of context parallel</li>\n<li><strong>cp_size:</strong>  size of context parallel</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>qo: inplace updated query, [len, bs, n_heads, 192] if not transpose\n  else  [bs, len, n_heads, 192]</li>\n  <li>ko: key output, [len, bs, n_heads, 192] if not transpose\n  else [bs, len, n_heads, 192]</li>\n  <li>vo: value output, [len, bs, n_heads, 128] if not transpose\n  else [bs, len, n_heads, 128]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">q</span>,</span><span class=\"param\">\t<span class=\"n\">kv</span>,</span><span class=\"param\">\t<span class=\"n\">k_pos_emb</span>,</span><span class=\"param\">\t<span class=\"n\">freqs</span>,</span><span class=\"param\">\t<span class=\"n\">mscale</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">transpose</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_q</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cu_seqlens_kv</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cp_rank</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">cp_size</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">reuse</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.scatter", "modulename": "linghe.utils.scatter", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.scatter.triton_aligned_scatter_add", "modulename": "linghe.utils.scatter", "qualname": "triton_aligned_scatter_add", "kind": "function", "doc": "<p>scatter_add for megatron 0.11</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>outputs:</strong>   output tensor</li>\n<li><strong>indices:</strong>   gather indices</li>\n<li><strong>weights:</strong>   rowwise weight, it is router prob in MoE router</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">outputs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weights</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.scatter.triton_scatter_add", "modulename": "linghe.utils.scatter", "qualname": "triton_scatter_add", "kind": "function", "doc": "<p>naive version of scatter add, very slow</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>outputs:</strong>  output tensor</li>\n<li><strong>indices:</strong>  indices</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">outputs</span>, </span><span class=\"param\"><span class=\"n\">indices</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.scatter.triton_unpermute_with_mask_map", "modulename": "linghe.utils.scatter", "qualname": "triton_unpermute_with_mask_map", "kind": "function", "doc": "<p>scatter add with row id map</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grad:</strong>  gradient tensor, [num_out_tokens, hidden_size]</li>\n<li><strong>row_id_map:</strong>  row id map, [n_experts, num_tokens]</li>\n<li><strong>probs:</strong>  [num_out_tokens]</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>output: [num_tokens, hidden_size]</li>\n  <li>restore_probs: [num_tokens, num_experts]</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">row_id_map</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">probs</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu", "modulename": "linghe.utils.silu", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.silu.triton_weighted_silu_forward", "modulename": "linghe.utils.silu", "qualname": "triton_weighted_silu_forward", "kind": "function", "doc": "<p>compute silu(x)*weight, used in bf16/fp16 training with MoE</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>weight:</strong>  tokenwise weight</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">weight</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">asm</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu.triton_weighted_silu_backward", "modulename": "linghe.utils.silu", "qualname": "triton_weighted_silu_backward", "kind": "function", "doc": "<p>backward of triton_weighted_silu_forward</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>g:</strong>  gradient tensor</li>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>weight:</strong>  weight tensor</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>dx: gradient of x</li>\n  <li>dw: gradient of weight</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">g</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">weight</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu.triton_silu_and_block_quant_forward", "modulename": "linghe.utils.silu", "qualname": "triton_silu_and_block_quant_forward", "kind": "function", "doc": "<p>fused silu and blockwise quantization, used in shared expert</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n<li><strong>output_mode:</strong>  one of {0, 1, 2}\n0: only output non-transposed quantized tensor\n1: only output transposed quantized tensor\n2: output both</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>out: quantized tensor</li>\n  <li>scale: quantization scale</li>\n  <li>transpose_output: quantized tensor of transposed output</li>\n  <li>transpose_scale: quantization scale of transposed output</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">output_mode</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu.triton_silu_and_block_quant_backward", "modulename": "linghe.utils.silu", "qualname": "triton_silu_and_block_quant_backward", "kind": "function", "doc": "<p>backward of triton_silu_and_block_quant_forward</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>g:</strong>  gradient</li>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>round_scale:</strong>  whether round to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>dx: quantized non-transposed gradient</li>\n  <li>dx_scale: scales of quantization non-transposed gradient</li>\n  <li>transpose_dx: quantized transposed gradient</li>\n  <li>transpose_dx_scale: scales of quantization transposed gradient</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">g</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu.triton_batch_weighted_silu_and_block_quant_forward", "modulename": "linghe.utils.silu", "qualname": "triton_batch_weighted_silu_and_block_quant_forward", "kind": "function", "doc": "<p>silu and blockwise quantize activation in routed experts</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  activation tensor in routed experts</li>\n<li><strong>weight:</strong>  router prob tensor</li>\n<li><strong>counts:</strong>  cuda tensor of token count per expert</li>\n<li><strong>splits:</strong>  python int list of token count per expert</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n<li><strong>output_mode:</strong>  one of {0, 1, 2}\n0: only output non-transposed quantized tensor\n1: only output transposed quantized tensor\n2: output both</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>out: quantized tensor</li>\n  <li>scale: quantization scale</li>\n  <li>transpose_output: quantized tensor of transposed output</li>\n  <li>transpose_scale: quantization scale of transposed output</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">weight</span>,</span><span class=\"param\">\t<span class=\"n\">counts</span>,</span><span class=\"param\">\t<span class=\"n\">splits</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"o\">=</span><span class=\"mi\">2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.silu.triton_batch_weighted_silu_and_block_quant_backward", "modulename": "linghe.utils.silu", "qualname": "triton_batch_weighted_silu_and_block_quant_backward", "kind": "function", "doc": "<p>backward of triton_batch_weighted_silu_and_block_quant_forward</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>g:</strong>  gradient</li>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>weight:</strong>  router prob tensor</li>\n<li><strong>counts:</strong>  cuda tensor of token count per expert</li>\n<li><strong>splits:</strong>  python int list of token count per expert</li>\n<li><strong>round_scale:</strong>  whether round scale to power of 2</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <ul>\n  <li>dx: quantized non-transposed gradient</li>\n  <li>dx_scale: scales of quantization non-transposed gradient</li>\n  <li>dw: gradient of weight</li>\n  <li>transpose_dx: quantized transposed gradient</li>\n  <li>transpose_dx_scale: scales of quantization transposed gradient</li>\n  </ul>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">g</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">weight</span>, </span><span class=\"param\"><span class=\"n\">counts</span>, </span><span class=\"param\"><span class=\"n\">splits</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">round_scale</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.topk", "modulename": "linghe.utils.topk", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.topk.triton_topk_forward", "modulename": "linghe.utils.topk", "qualname": "triton_topk_forward", "kind": "function", "doc": "<p>calculate topk.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor.</li>\n<li><strong>k:</strong>  topk</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>values: topk values \n  indices: topk indices</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">k</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.topk.triton_topk_backward", "modulename": "linghe.utils.topk", "qualname": "triton_topk_backward", "kind": "function", "doc": "<p>topk backward.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grad_output:</strong>  grad tensor of values.</li>\n<li><strong>indices:</strong>  topk indices</li>\n<li><strong>N:</strong>  dim</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dx</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"n\">indices</span>, </span><span class=\"param\"><span class=\"n\">N</span>, </span><span class=\"param\"><span class=\"n\">dim</span><span class=\"o\">=-</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.topk.triton_group_topk_score_forward", "modulename": "linghe.utils.topk", "qualname": "triton_group_topk_score_forward", "kind": "function", "doc": "<p>calculate topk.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor.</li>\n<li><strong>expert_bias:</strong>  expert bias</li>\n<li><strong>k:</strong>  topk</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>probs:<br />\n  routing_map: \n  tokens_per_expert:</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">k</span>,</span><span class=\"param\">\t<span class=\"n\">expert_bias</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">num_groups</span><span class=\"o\">=</span><span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">group_topk</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">scaling_factor</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">score_function</span><span class=\"o\">=</span><span class=\"s1\">&#39;sigmoid&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-20</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.topk.triton_group_topk_score_backward", "modulename": "linghe.utils.topk", "qualname": "triton_group_topk_score_backward", "kind": "function", "doc": "<p>topk backward.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>grad_output:</strong>  grad tensor of prob.</li>\n<li><strong>routing_map:</strong>  topk indices</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>dx: grad of logits</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">grad_output</span>, </span><span class=\"param\"><span class=\"nb\">input</span>, </span><span class=\"param\"><span class=\"n\">routing_map</span>, </span><span class=\"param\"><span class=\"n\">scaling_factor</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>, </span><span class=\"param\"><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">1e-20</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.transpose", "modulename": "linghe.utils.transpose", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.transpose.triton_transpose", "modulename": "linghe.utils.transpose", "qualname": "triton_transpose", "kind": "function", "doc": "<p>transpose x with dim0 and dim1</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>inner:</strong>  inner dim if True, outer dim if False </li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>transposed tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">inner</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.transpose.triton_transpose_and_pad", "modulename": "linghe.utils.transpose", "qualname": "triton_transpose_and_pad", "kind": "function", "doc": "<p>transpose x and padding the column size to be mutiplier of 32,\nit is used for calculated gradient of weight with torch._scaled__mm</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  input tensor</li>\n<li><strong>out:</strong> </li>\n<li><strong>pad:</strong>  whether need padding</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>out: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">out</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.transpose.triton_batch_transpose", "modulename": "linghe.utils.transpose", "qualname": "triton_batch_transpose", "kind": "function", "doc": "<p>batch transpose x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  input tensor list, [M, N]*expert</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>xts: output tensor list, [N,M]*expert</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"n\">xts</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.transpose.triton_batch_transpose_and_pad", "modulename": "linghe.utils.transpose", "qualname": "triton_batch_transpose_and_pad", "kind": "function", "doc": "<p>transpose and pad each tensor stored in x</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  [sum(bs), N]</li>\n<li><strong>count_list:</strong>  a python list of token count</li>\n<li><strong>pad:</strong>  whether pad to mutiplier of 32,\npadding value should be filled with 0 if padded</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>x_t: output tensor</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">count_list</span>, </span><span class=\"param\"><span class=\"n\">x_t</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">pad</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "linghe.utils.unary", "modulename": "linghe.utils.unary", "kind": "module", "doc": "<p>Copyright (c) Ant Financial Service Group and its affiliates.</p>\n"}, {"fullname": "linghe.utils.unary.triton_batch_clip", "modulename": "linghe.utils.unary", "qualname": "triton_batch_clip", "kind": "function", "doc": "<p>return [clip(x, -clip_value, clip_value) for x in xs],\nused to clip gradient.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>xs:</strong>  Tensor lists.</li>\n<li><strong>clip_value:</strong>  a python float scale</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>updated xs</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">xs</span>, </span><span class=\"param\"><span class=\"n\">clip_value</span><span class=\"o\">=</span><span class=\"mf\">100.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();